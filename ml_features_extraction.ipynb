{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd13bf5",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "ML and DL models need to be fed with features. In this notebook, we will extract some characteristics from audio files that can be used to train these kinds of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae6ca8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from scipy.signal import get_window\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 2706"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d9d4f4",
   "metadata": {},
   "source": [
    "We have built a metadata file that will help us with the feature extraction. However, for representational issues we will delete the rows corresponding with the emotions 'Calm' and 'Surprised'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8527a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Path</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Emotional_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CREMA-D</td>\n",
       "      <td>./audio_data/crema-d/1001_DFA_ANG_XX.wav</td>\n",
       "      <td>Angry</td>\n",
       "      <td>Male</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CREMA-D</td>\n",
       "      <td>./audio_data/crema-d/1001_DFA_DIS_XX.wav</td>\n",
       "      <td>Disgust</td>\n",
       "      <td>Male</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CREMA-D</td>\n",
       "      <td>./audio_data/crema-d/1001_DFA_FEA_XX.wav</td>\n",
       "      <td>Fearful</td>\n",
       "      <td>Male</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CREMA-D</td>\n",
       "      <td>./audio_data/crema-d/1001_DFA_HAP_XX.wav</td>\n",
       "      <td>Happy</td>\n",
       "      <td>Male</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CREMA-D</td>\n",
       "      <td>./audio_data/crema-d/1001_DFA_NEU_XX.wav</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Male</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9907</th>\n",
       "      <td>MESD</td>\n",
       "      <td>./audio_data/mesd/Sadness_M_B_ira.wav</td>\n",
       "      <td>Sad</td>\n",
       "      <td>Male</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9908</th>\n",
       "      <td>MESD</td>\n",
       "      <td>./audio_data/mesd/Sadness_M_B_llanto.wav</td>\n",
       "      <td>Sad</td>\n",
       "      <td>Male</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909</th>\n",
       "      <td>MESD</td>\n",
       "      <td>./audio_data/mesd/Sadness_M_B_locura.wav</td>\n",
       "      <td>Sad</td>\n",
       "      <td>Male</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9910</th>\n",
       "      <td>MESD</td>\n",
       "      <td>./audio_data/mesd/Sadness_M_B_metralleta.wav</td>\n",
       "      <td>Sad</td>\n",
       "      <td>Male</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9911</th>\n",
       "      <td>MESD</td>\n",
       "      <td>./audio_data/mesd/Sadness_M_B_tristeza.wav</td>\n",
       "      <td>Sad</td>\n",
       "      <td>Male</td>\n",
       "      <td>XX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9528 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dataset                                          Path  Emotion   Sex  \\\n",
       "0     CREMA-D      ./audio_data/crema-d/1001_DFA_ANG_XX.wav    Angry  Male   \n",
       "1     CREMA-D      ./audio_data/crema-d/1001_DFA_DIS_XX.wav  Disgust  Male   \n",
       "2     CREMA-D      ./audio_data/crema-d/1001_DFA_FEA_XX.wav  Fearful  Male   \n",
       "3     CREMA-D      ./audio_data/crema-d/1001_DFA_HAP_XX.wav    Happy  Male   \n",
       "4     CREMA-D      ./audio_data/crema-d/1001_DFA_NEU_XX.wav  Neutral  Male   \n",
       "...       ...                                           ...      ...   ...   \n",
       "9907     MESD         ./audio_data/mesd/Sadness_M_B_ira.wav      Sad  Male   \n",
       "9908     MESD      ./audio_data/mesd/Sadness_M_B_llanto.wav      Sad  Male   \n",
       "9909     MESD      ./audio_data/mesd/Sadness_M_B_locura.wav      Sad  Male   \n",
       "9910     MESD  ./audio_data/mesd/Sadness_M_B_metralleta.wav      Sad  Male   \n",
       "9911     MESD    ./audio_data/mesd/Sadness_M_B_tristeza.wav      Sad  Male   \n",
       "\n",
       "     Emotional_level  \n",
       "0                 XX  \n",
       "1                 XX  \n",
       "2                 XX  \n",
       "3                 XX  \n",
       "4                 XX  \n",
       "...              ...  \n",
       "9907              XX  \n",
       "9908              XX  \n",
       "9909              XX  \n",
       "9910              XX  \n",
       "9911              XX  \n",
       "\n",
       "[9528 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audios = pd.read_csv(\"./audio_data/complete_metadata.csv\")\n",
    "audios = audios[~((audios['Emotion'] == 'Calm') | (audios['Emotion'] == 'Surprised'))]\n",
    "audios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b28c65",
   "metadata": {},
   "source": [
    "Now we have to make some adjustment to data to standardise them. We have to make the audios be sampled at the same rate, normalise them (since they may not have been recorded at the same conditions) and unify the channel. Duration will not be changed for reasons explained later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78286f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(path: str, target_sr: int = 16000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load and preprocess an audio file.\n",
    "\n",
    "    Steps:\n",
    "        - Load audio as mono\n",
    "        - Resample to target_sr\n",
    "        - Normalize by RMS energy\n",
    "        - (Optionally) adjust to a fixed duration with trimming or padding\n",
    "\n",
    "    Args:\n",
    "        path: Path to the audio file.\n",
    "        target_sr: Target sampling rate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed audio signal.\n",
    "    \"\"\"\n",
    "    # Load audio and force mono channel\n",
    "    audio, sr = librosa.load(path, sr=None, mono=True)\n",
    "\n",
    "    # Resample if the sampling rate does not match target_sr\n",
    "    if sr != target_sr:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "        sr = target_sr\n",
    "\n",
    "    # Normalize using RMS energy to ensure consistent loudness\n",
    "    rms = np.sqrt(np.mean(audio**2))\n",
    "    if rms > 0:\n",
    "        audio = audio / rms\n",
    "\n",
    "    return audio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35234d97",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2986ee6",
   "metadata": {},
   "source": [
    "Now, let's start to extract some features from the audios. We will explore the following propierties:\n",
    "* Total Spectrum Power: $\\log{\\int_0^{w_0}|F(\\omega)|^2\\,d\\omega}$, where $|F(\\omega)|^2$ is the power at frequency $\\omega$ Hz and $\\omega_0$ is half the sampling rate.\n",
    "* Subband Powers: For intervals $[0, \\omega_0/8],~[\\omega_0/8, \\omega_0/4],~[\\omega_0/4, \\omega_0/2],~[\\omega_0/2, \\omega_0]$, let $L_j,~H_j$ be the intervals extreme values for $j\\in\\{1,2,3,4\\}$. Then, the subband powers are defined as the spectrum power for each interval.\n",
    "* Brightness: Is the spectrum centroid, defined as $\\omega_C=\\int_0^{\\omega_0}\\omega\\cdot|F(\\omega)|^2\\,d\\omega\\left/\\int_0^{\\omega_0}|F(\\omega)|^2\\,d\\omega\\right.$.\n",
    "* Bandwidth: $B=\\sqrt{\\int_0^{\\omega_0}(\\omega-\\omega_0)\\cdot|F(\\omega)|^2\\,d\\omega}\\left/\\sqrt{\\int_0^{\\omega_0}|F(\\omega)|^2\\,d\\omega}\\right.$.\n",
    "* Pitch Frequency: $P(x)=x$ if $x>P_0$ and $P(x)=0$ in any other case, for a fixed $P_0$ chosen empirically.\n",
    "* MFCC: Coefficients of the Fast Fourier Transform filtered by the Mel function. We will be using Librosa to extract them.\n",
    "* Zero-crossing rate: We will be using Librosa for this one too.\n",
    "\n",
    "Since we are processing audio signals, their properties change within its duration, because they are not stationary waves. In order to capture better the features, we will use Hamming windows. This is why we do not need to pad the audios with silence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a4f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Spectrum Power\n",
    "def total_spectrum_power(signal: np.ndarray, **kwargs) -> float:\n",
    "    \"\"\"\n",
    "    Compute the total spectral power of a signal.\n",
    "\n",
    "    The function calculates the power spectral density using the FFT,\n",
    "    integrates it across frequencies, and returns the logarithm of\n",
    "    the total spectral power.\n",
    "\n",
    "    Args:\n",
    "        signal: Input audio signal.\n",
    "        **kwargs: Optional keyword arguments.\n",
    "            - sr (int): Sampling rate of the signal (default is 16000).\n",
    "\n",
    "    Returns:\n",
    "        float: Logarithm of the total spectral power.\n",
    "    \"\"\"\n",
    "    sr = kwargs.get(\"sr\", 16000)\n",
    "\n",
    "    # Compute spectral power density using the real FFT\n",
    "    power = np.abs(np.fft.rfft(signal))**2\n",
    "\n",
    "    # Frequency bins corresponding to the FFT\n",
    "    freq = np.fft.rfftfreq(n=len(signal), d=1 / sr)\n",
    "\n",
    "    # Frequency resolution (delta between adjacent frequency bins)\n",
    "    delta_freq = freq[1] - freq[0]\n",
    "\n",
    "    # Integrate spectral power over all frequencies\n",
    "    sigma = np.sum(power) * delta_freq\n",
    "\n",
    "    # Return log of total power with numerical stability\n",
    "    return np.log(sigma + 1e-10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a3d0708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subband powers\n",
    "def subband_powers(signal: np.ndarray, n_intervals: int = 4, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Compute logarithmic power for multiple frequency subbands.\n",
    "\n",
    "    The signal spectrum is divided into logarithmically spaced subbands\n",
    "    up to the Nyquist frequency (sr/2). For each subband, the power is\n",
    "    computed by integrating the spectral density, and the logarithm of\n",
    "    that power is returned.\n",
    "\n",
    "    Args:\n",
    "        signal: Input audio signal.\n",
    "        n_intervals: Number of logarithmic subbands to compute (default is 4).\n",
    "        **kwargs: Optional keyword arguments.\n",
    "            - sr (int): Sampling rate of the signal (default is 16000).\n",
    "            - w_0 (float): Maximum frequency limit (default is sr/2).\n",
    "\n",
    "    Returns:\n",
    "        list[float]: Logarithmic power of each subband.\n",
    "    \"\"\"\n",
    "    sr = kwargs.get(\"sr\", 16000)\n",
    "    w_0 = kwargs.get(\"w_0\", sr / 2)\n",
    "\n",
    "    # Define logarithmically spaced frequency intervals\n",
    "    intervals = (\n",
    "        [(0, w_0 / 2 ** (n_intervals - 1))]\n",
    "        + [\n",
    "            (w_0 / 2 ** (n_intervals - i), w_0 / 2 ** (n_intervals - i - 1))\n",
    "            for i in range(1, n_intervals)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Compute spectral power density\n",
    "    power = np.abs(np.fft.rfft(signal))**2\n",
    "\n",
    "    # Frequency bins corresponding to FFT\n",
    "    freq = np.fft.rfftfreq(n=len(signal), d=1 / sr)\n",
    "\n",
    "    log_band_power = []\n",
    "    for (L_i, H_i) in intervals:\n",
    "        # Select frequency bins within the subband\n",
    "        band_mask = (freq >= L_i) & (freq <= H_i)\n",
    "\n",
    "        # Frequency resolution\n",
    "        delta_f = freq[1] - freq[0]\n",
    "\n",
    "        # Integrate spectral power within subband\n",
    "        band_power = np.sum(power[band_mask]) * delta_f\n",
    "\n",
    "        # Store logarithmic subband power with numerical stability\n",
    "        log_band_power.append(np.log(band_power + 1e-10))\n",
    "\n",
    "    return log_band_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dbd8531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brightness\n",
    "def brightness(signal: np.ndarray, **kwargs) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the spectral centroid of a signal, a measure of its \"brightness.\"\n",
    "\n",
    "    The spectral centroid represents the center of mass of the spectrum,\n",
    "    which correlates with the perceived brightness of the sound.\n",
    "\n",
    "    Args:\n",
    "        signal: Input audio signal.\n",
    "        **kwargs: Optional keyword arguments.\n",
    "            - sr (int): Sampling rate of the signal (default is 16000).\n",
    "            - n_fft (int): Length of the FFT window (default is 512).\n",
    "            - hop_length (int): Number of samples between successive frames (default is 160).\n",
    "            - window (str or tuple or np.ndarray): Window function for FFT (default is \"hann\").\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Spectral centroid of the signal (shape: [1, n_frames]).\n",
    "    \"\"\"\n",
    "    # Compute spectral centroid using librosa\n",
    "    return librosa.feature.spectral_centroid(\n",
    "        y=signal,\n",
    "        sr=kwargs.get(\"sr\", 16000),\n",
    "        n_fft=kwargs.get(\"n_fft\", 512),\n",
    "        hop_length=kwargs.get(\"hop_length\", 160),\n",
    "        window=kwargs.get(\"window\", \"hann\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be754a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandwidth\n",
    "def bandwidth(signal: np.ndarray, **kwargs) -> float:\n",
    "    \"\"\"\n",
    "    Compute the spectral bandwidth of a signal.\n",
    "\n",
    "    The spectral bandwidth is a measure of the spread of the spectrum\n",
    "    around the origin, weighted by frequency and spectral power.\n",
    "\n",
    "    Args:\n",
    "        signal: Input audio signal.\n",
    "        **kwargs: Optional keyword arguments.\n",
    "            - sr (int): Sampling rate of the signal (default is 16000).\n",
    "            - w_0 (float): Maximum frequency for normalization (default is sr/2).\n",
    "\n",
    "    Returns:\n",
    "        float: Spectral bandwidth of the signal.\n",
    "    \"\"\"\n",
    "    sr = kwargs.get(\"sr\", 16000)\n",
    "    w_0 = kwargs.get(\"w_0\", sr / 2)\n",
    "\n",
    "    # Compute spectral power density\n",
    "    power = np.abs(np.fft.rfft(signal))**2\n",
    "\n",
    "    # Frequency bins corresponding to the FFT\n",
    "    freq = np.fft.rfftfreq(n=len(signal), d=1 / sr)\n",
    "\n",
    "    # Frequency resolution (spacing between FFT bins)\n",
    "    delta_freq = freq[1] - freq[0]\n",
    "\n",
    "    # Compute weighted spectral spread and normalize by w_0\n",
    "    sigma = np.sum(freq * power) * delta_freq * w_0\n",
    "\n",
    "    # Return the square root of the weighted sum as bandwidth\n",
    "    return np.sqrt(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d76274fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitch\n",
    "def pitch(signal: np.ndarray, **kwargs) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Estimate the pitch (fundamental frequency) of an audio signal using autocorrelation.\n",
    "\n",
    "    The function frames the signal, removes the DC component from each frame,\n",
    "    computes the normalized autocorrelation, and estimates the pitch for each frame\n",
    "    based on the location of the maximum autocorrelation within a valid period range.\n",
    "\n",
    "    Args:\n",
    "        signal: Input audio signal.\n",
    "        **kwargs: Optional keyword arguments.\n",
    "            - sr (int): Sampling rate of the signal (default is 16000).\n",
    "            - n_fft (int): Frame length for analysis (default is 512).\n",
    "            - hop_length (int): Hop length between frames (default is 160).\n",
    "            - threshold (float): Minimum normalized autocorrelation to consider a valid pitch (default is 0.3).\n",
    "            - min_freq (float): Minimum detectable frequency in Hz (default is 50).\n",
    "            - max_freq (float): Maximum detectable frequency in Hz (default is 500).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Estimated pitch track (Hz) for each frame. Frames with no detected pitch are 0.\n",
    "    \"\"\"\n",
    "    sr = kwargs.get(\"sr\", 16000)\n",
    "    frame_length = kwargs.get(\"n_fft\", 512)\n",
    "    hop_length = kwargs.get(\"hop_length\", 160)\n",
    "    threshold = kwargs.get(\"threshold\", 0.3)\n",
    "    min_freq = kwargs.get(\"min_freq\", 50)\n",
    "    max_freq = kwargs.get(\"max_freq\", 500)\n",
    "\n",
    "    # Convert frequency limits to period limits in samples\n",
    "    min_period = int(sr / max_freq)\n",
    "    max_period = int(sr / min_freq)\n",
    "\n",
    "    # Frame the signal for analysis\n",
    "    frames = librosa.util.frame(signal, frame_length=frame_length, hop_length=hop_length).T\n",
    "    pitch_track = []\n",
    "\n",
    "    for frame in frames:\n",
    "        # Remove DC component\n",
    "        frame = frame - np.mean(frame)\n",
    "\n",
    "        # Skip silent frames\n",
    "        if np.all(frame == 0):\n",
    "            pitch_track.append(0)\n",
    "            continue\n",
    "\n",
    "        # Compute normalized autocorrelation\n",
    "        r = librosa.autocorrelate(frame)\n",
    "        r = r / (np.max(np.abs(r)) + 1e-9)\n",
    "\n",
    "        # Ignore lags outside the valid pitch range\n",
    "        r[:min_period] = 0\n",
    "        r[max_period:] = 0\n",
    "\n",
    "        # Find the lag corresponding to maximum autocorrelation\n",
    "        T = np.argmax(r)\n",
    "        p = r[T]\n",
    "\n",
    "        # Estimate pitch if autocorrelation exceeds threshold\n",
    "        pitch_value = 1 / T if p > threshold else 0\n",
    "        pitch_track.append(pitch_value)\n",
    "\n",
    "    return np.array(pitch_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "694267ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCC\n",
    "def mfcc(signal: np.ndarray, n_mfcc: int = 20, **kwargs) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the mean and std of Mel-frequency cepstral coefficients (MFCCs) of an audio signal.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Concatenated mean and std MFCC coefficients (shape: [2 * n_mfcc]).\n",
    "    \"\"\"\n",
    "    # Compute MFCCs using librosa\n",
    "    mfccs = librosa.feature.mfcc(\n",
    "        y=signal,\n",
    "        sr=kwargs.get(\"sr\", 16000),\n",
    "        n_mfcc=n_mfcc,\n",
    "        n_fft=kwargs.get(\"n_fft\", 512),\n",
    "        hop_length=kwargs.get(\"hop_length\", 160),\n",
    "        window=kwargs.get(\"window\", \"hann\")\n",
    "    )\n",
    "\n",
    "    # Transpose to shape (frames, n_mfcc)\n",
    "    mfccs_T = mfccs.T\n",
    "\n",
    "    # Compute mean and std across frames\n",
    "    mean = np.mean(mfccs_T, axis=0)\n",
    "    std = np.std(mfccs_T, axis=0)\n",
    "\n",
    "    return {'Mean': mean, \"SD\": std}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba3b7fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-crossing rate\n",
    "def zeros_rate(signal: np.ndarray, **kwargs) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the zero-crossing rate of an audio signal.\n",
    "\n",
    "    The zero-crossing rate measures the rate at which the signal changes\n",
    "    sign, which is often used as a simple measure of signal noisiness or\n",
    "    percussiveness.\n",
    "\n",
    "    Args:\n",
    "        signal: Input audio signal.\n",
    "        **kwargs: Optional keyword arguments.\n",
    "            - frame_length (int): Length of the analysis frame (default is 400).\n",
    "            - hop_length (int): Number of samples between successive frames (default is 160).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Zero-crossing rate for each frame (shape: [1, n_frames]).\n",
    "    \"\"\"\n",
    "    # Compute zero-crossing rate using librosa\n",
    "    return librosa.feature.zero_crossing_rate(\n",
    "        y=signal,\n",
    "        frame_length=kwargs.get(\"frame_length\", 400),\n",
    "        hop_length=kwargs.get(\"hop_length\", 160)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66fbc200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma_stft\n",
    "def chroma(signal: np.ndarray, **kwargs) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the mean chroma feature of an audio signal.\n",
    "\n",
    "    Chroma features represent the energy distribution across the 12\n",
    "    pitch classes of the musical octave, often used in music analysis\n",
    "    and key detection.\n",
    "\n",
    "    Args:\n",
    "        signal: Input audio signal.\n",
    "        **kwargs: Optional keyword arguments.\n",
    "            - sr (int): Sampling rate of the signal (default is 16000).\n",
    "            - n_fft (int): FFT window size (default is 512).\n",
    "            - hop_length (int): Hop length between frames (default is 160).\n",
    "            - window (str or tuple or np.ndarray): Window function for FFT (default is \"hann\").\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Mean chroma features across all frames (shape: [12]).\n",
    "    \"\"\"\n",
    "    # Compute magnitude STFT\n",
    "    stft = np.abs(librosa.stft(signal))\n",
    "\n",
    "    # Compute chroma features from the STFT\n",
    "    chroma_features = librosa.feature.chroma_stft(\n",
    "        S=stft,\n",
    "        sr=kwargs.get(\"sr\", 16000),\n",
    "        n_fft=kwargs.get(\"n_fft\", 512),\n",
    "        hop_length=kwargs.get(\"hop_length\", 160),\n",
    "        window=kwargs.get(\"window\", \"hann\")\n",
    "    )\n",
    "\n",
    "    # Return mean chroma across frames\n",
    "    return {\"Mean\": np.mean(chroma_features.T, axis=0), \"SD\": np.std(chroma_features.T, axis=0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85f66a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mel Spectrogram\n",
    "def mel_spectro(signal: np.ndarray, **kwargs) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the mean Mel-spectrogram of an audio signal.\n",
    "\n",
    "    The Mel-spectrogram represents the signal's short-term power spectrum\n",
    "    mapped onto the Mel scale, which is perceptually motivated. Useful\n",
    "    for speech and audio analysis.\n",
    "\n",
    "    Args:\n",
    "        signal: Input audio signal.\n",
    "        **kwargs: Optional keyword arguments.\n",
    "            - sr (int): Sampling rate of the signal (default is 16000).\n",
    "            - n_fft (int): FFT window size (default is 512).\n",
    "            - hop_length (int): Hop length between frames (default is 160).\n",
    "            - window (str or tuple or np.ndarray): Window function for FFT (default is \"hann\").\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Mean Mel-spectrogram across all frames.\n",
    "    \"\"\"\n",
    "    # Compute Mel-spectrogram using librosa\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=signal,\n",
    "        sr=kwargs.get(\"sr\", 16000),\n",
    "        n_fft=kwargs.get(\"n_fft\", 512),\n",
    "        hop_length=kwargs.get(\"hop_length\", 160),\n",
    "        window=kwargs.get(\"window\", \"hann\")\n",
    "    )\n",
    "\n",
    "    # Return mean Mel-spectrogram across frames\n",
    "    return {\"Mean\": np.mean(mel_spec.T, axis=0), \"SD\": np.std(mel_spec.T, axis=0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "806adefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hamming window for spectral power\n",
    "def frame_signal(signal: np.ndarray, frame_length: int, hop_length: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Split a signal into overlapping frames using stride tricks.\n",
    "\n",
    "    Args:\n",
    "        signal: Input audio signal.\n",
    "        frame_length: Number of samples per frame.\n",
    "        hop_length: Number of samples to advance between frames.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 2D array of frames with shape (n_frames, frame_length).\n",
    "    \"\"\"\n",
    "    n_frames = 1 + (len(signal) - frame_length) // hop_length\n",
    "\n",
    "    # Use numpy stride tricks to create a 2D view of overlapping frames\n",
    "    frames = np.lib.stride_tricks.as_strided(\n",
    "        signal,\n",
    "        shape=(n_frames, frame_length),\n",
    "        strides=(signal.strides[0] * hop_length, signal.strides[0])\n",
    "    )\n",
    "\n",
    "    return frames\n",
    "\n",
    "\n",
    "def apply_function_per_frame(signal: np.ndarray, func, **kwargs) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply a feature extraction function to each frame of a signal.\n",
    "\n",
    "    The signal is split into overlapping frames, optionally windowed,\n",
    "    and the given function is applied to each frame.\n",
    "\n",
    "    Args:\n",
    "        signal: Input audio signal.\n",
    "        func: Function to apply to each frame.\n",
    "        **kwargs: Optional keyword arguments.\n",
    "            - frame_length (int): Number of samples per frame.\n",
    "            - hop_length (int): Number of samples between frames.\n",
    "            - window (str): Type of window to apply to each frame (default is \"hann\").\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of features computed per frame.\n",
    "    \"\"\"\n",
    "    frame_length = kwargs.get(\"frame_length\")\n",
    "    hop_length = kwargs.get(\"hop_length\")\n",
    "    window_type = kwargs.get(\"window\", \"hann\")\n",
    "\n",
    "    # Split signal into frames\n",
    "    frames = frame_signal(signal, frame_length, hop_length)\n",
    "\n",
    "    # Create window to apply to each frame\n",
    "    hamming_window = get_window(window_type, frame_length, fftbins=True)\n",
    "\n",
    "    features = []\n",
    "    for frame in frames:\n",
    "        # Apply window to frame and compute feature\n",
    "        windowed = frame * hamming_window\n",
    "        features.append(func(windowed, **kwargs))\n",
    "\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256448ad",
   "metadata": {},
   "source": [
    "Finally, we can extract the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56abca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(signal: np.ndarray, params: dict) -> list:\n",
    "    \"\"\"\n",
    "    Extract a comprehensive set of audio features from a signal.\n",
    "\n",
    "    Features extracted:\n",
    "        - Total spectral power\n",
    "        - Subband powers\n",
    "        - Spectral brightness\n",
    "        - Spectral bandwidth\n",
    "        - Pitch track\n",
    "        - MFCCs\n",
    "        - Zero-crossing rate\n",
    "        - Chroma features\n",
    "        - Mel-spectrogram\n",
    "\n",
    "    Args:\n",
    "        signal: Input audio signal.\n",
    "        params: Dictionary of optional parameters for feature extraction functions,\n",
    "            such as 'sr', 'frame_length', 'hop_length', 'n_fft', 'window', etc.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing arrays for each feature in the order listed above.\n",
    "    \"\"\"\n",
    "    # Compute features per frame or for the entire signal\n",
    "    total_power = apply_function_per_frame(signal, total_spectrum_power, **params)\n",
    "    subband_power = apply_function_per_frame(signal, subband_powers, **params)\n",
    "    bright = brightness(signal, **params)\n",
    "    band = apply_function_per_frame(signal, bandwidth, **params)\n",
    "    pitches = pitch(signal, **params)\n",
    "    coeff = mfcc(signal, n_mfcc=10, **params)\n",
    "    zero_crossing = zeros_rate(signal, **params)\n",
    "    chroma_stft = chroma(signal, **params)\n",
    "    mel = mel_spectro(signal, **params)\n",
    "\n",
    "    return [\n",
    "        total_power,\n",
    "        subband_power,\n",
    "        bright,\n",
    "        band,\n",
    "        pitches,\n",
    "        coeff,\n",
    "        zero_crossing,\n",
    "        chroma_stft,\n",
    "        mel\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c904ee8",
   "metadata": {},
   "source": [
    "To improve the performance of the models we will apply data augmentation. For each *selected* file, we will add some noise, strect its duration and lightly shift its pitch. It is important to explain that *selection* is necessary to avoid any data leakage: if we were to train with augmented files generated from test data, we would be learning something about test data, which is not the correct way to proceed. Thus, before extracting the features we will split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de5b5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== AUGMENTATIONS ===============\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.01, p=0.5),\n",
    "    TimeStretch(min_rate=0.9, max_rate=1.1, p=0.5),\n",
    "    PitchShift(min_semitones=-2, max_semitones=2, p=0.5),\n",
    "])\n",
    "# ==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a16ede33",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"w_0\": 8000,\n",
    "    \"sr\": 16000,\n",
    "    \"frame_length\": 400,\n",
    "    \"hop_length\": 160,\n",
    "    \"n_fft\": 400,\n",
    "    \"window\": \"hamming\"\n",
    "}\n",
    "\n",
    "NUM_AUGMENTATIONS = 2  # cuántas veces aumentar cada audio\n",
    "AUGMENTATION_ENABLED = True\n",
    "SAVE_PATH = \"./features/ML_features.pickle\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3941b",
   "metadata": {},
   "source": [
    "At this moment we have defined everything needed. We proceed then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45e937ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 1. Train & Test split\n",
    "# --------------------------------------------------\n",
    "def stratified_split(\n",
    "    df: \"pd.DataFrame\", y_col: str, test_frac: float = 0.3, seed: int = SEED\n",
    ") -> \"pd.DataFrame\":\n",
    "    \"\"\"\n",
    "    Perform a stratified train-test split on a DataFrame.\n",
    "\n",
    "    Ensures that each class in `y_col` is represented proportionally\n",
    "    in both the training and test sets.\n",
    "\n",
    "    Args:\n",
    "        df: Input DataFrame containing features and target column.\n",
    "        y_col: Name of the target column used for stratification.\n",
    "        test_frac: Fraction of samples to include in the test set (default is 0.2).\n",
    "        seed: Random seed for reproducibility (default is 42).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an added 'split' column containing\n",
    "                      values 'train' or 'test' for each row.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Shuffle the DataFrame\n",
    "    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    test_idx = []\n",
    "\n",
    "    # Determine test indices for each class to preserve proportions\n",
    "    for label, g in df.groupby(y_col):\n",
    "        n_test = max(1, int(round(len(g) * test_frac)))\n",
    "        test_idx.extend(g.sample(n=n_test, random_state=seed).index.tolist())\n",
    "\n",
    "    # Initialize all rows as training set\n",
    "    df[\"split\"] = \"train\"\n",
    "\n",
    "    # Assign test rows\n",
    "    df.loc[test_idx, \"split\"] = \"test\"\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "audios = stratified_split(audios, y_col=\"Emotion\", test_frac=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7121d8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faler\\anaconda3\\envs\\dl_python\\Lib\\site-packages\\librosa\\core\\pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 2. Feature extraction & Augmentations (Train)\n",
    "# --------------------------------------------------\n",
    "rows = []\n",
    "for _, row in audios.iterrows():\n",
    "    path, emotion, sex, split = row[\"Path\"], row[\"Emotion\"], row[\"Sex\"], row[\"split\"]\n",
    "    signal = process_audio(path, target_sr=params[\"sr\"])\n",
    "\n",
    "    n_repeats = NUM_AUGMENTATIONS + 1 if (AUGMENTATION_ENABLED and split == \"train\") else 1\n",
    "    for i in range(n_repeats):\n",
    "        signal_aug = signal if i == 0 else augment(samples=signal, sample_rate=params[\"sr\"])\n",
    "        (total_power, subband_power, bright, band, pitches, coeff, zero_crossing,\n",
    "         chroma_stft, mel) = extract(signal_aug, params)\n",
    "\n",
    "        rows.append({\n",
    "            \"Emotion\": emotion,\n",
    "            \"Sex\": sex,\n",
    "            \"Split\": split,\n",
    "            \"Augmented\": i > 0,\n",
    "            \"Total_Spectrum_Power_mean\": np.mean(total_power),\n",
    "            \"Total_Spectrum_Power_sd\": np.std(total_power),\n",
    "            \"Subband_Power_1_mean\": np.mean([f[0] for f in subband_power]),\n",
    "            \"Subband_Power_1_sd\": np.std([f[0] for f in subband_power]),\n",
    "            \"Subband_Power_2_mean\": np.mean([f[1] for f in subband_power]),\n",
    "            \"Subband_Power_2_sd\": np.std([f[1] for f in subband_power]),\n",
    "            \"Subband_Power_3_mean\": np.mean([f[2] for f in subband_power]),\n",
    "            \"Subband_Power_3_sd\": np.std([f[2] for f in subband_power]),\n",
    "            \"Subband_Power_4_mean\": np.mean([f[3] for f in subband_power]),\n",
    "            \"Subband_Power_4_sd\": np.std([f[3] for f in subband_power]),\n",
    "            \"Brightness_mean\": np.mean(bright),\n",
    "            \"Brightness_sd\": np.std(bright),\n",
    "            \"Bandwidth_mean\": np.mean(band) if isinstance(band, (np.ndarray, list)) else band,\n",
    "            \"Bandwidth_sd\": np.std(band) if isinstance(band, (np.ndarray, list)) else 0,\n",
    "            \"Pitch_mean\": np.mean(pitches[pitches > 0]) if np.any(pitches > 0) else 0,\n",
    "            \"Pitch_sd\": np.std(pitches[pitches > 0]) if np.any(pitches > 0) else 0,\n",
    "            \"Zero_crossing_rate_mean\": np.mean(zero_crossing),\n",
    "            \"Zero_crossing_rate_sd\": np.std(zero_crossing),\n",
    "            \"MFCC_mean\": coeff['Mean'],\n",
    "            \"MFCC_sd\": coeff['SD'],\n",
    "            \"Chromogram_mean\": chroma_stft['Mean'],\n",
    "            \"Chromogram_sd\": chroma_stft['SD'],\n",
    "            \"Mel_spectrogram_mean\": mel['Mean'],\n",
    "            \"Mel_spectrogram_sd\": mel['SD']\n",
    "        })\n",
    "\n",
    "features_augmented = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a135474",
   "metadata": {},
   "source": [
    "This features will be saved in a pickle file. They are in need for processing (for example, MFCC is winded). We will treat them later when preparing model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b64e9f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SAVE_PATH, \"wb\") as f:\n",
    "    pickle.dump(features_augmented, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
